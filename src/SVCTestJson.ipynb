{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'OAS30001': [['../files/OAS30001_MR_d0129/anat3/sub-OAS30001_sess-d0129_run-02_T1w.nii.gz', '../files/OAS30001_MR_d0129/anat2/sub-OAS30001_sess-d0129_run-01_T1w.nii.gz'], ['../files/OAS30001_MR_d0757/anat3/sub-OAS30001_sess-d0757_run-02_T1w.nii.gz', '../files/OAS30001_MR_d0757/anat2/sub-OAS30001_sess-d0757_run-01_T1w.nii.gz']]}\n",
      "['Dementia' 'NoDementia']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:275: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5688,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [6217, 5688]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f41cabfec974>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;31m#fit decoder, predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconditions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m \u001b[0manova_svc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconditions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manova_svc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \"\"\"\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 Xt, fitted_transformer = fit_transform_one_cached(\n\u001b[1;32m    212\u001b[0m                     \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                     **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    214\u001b[0m                 \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, weight, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m                        **fit_params):\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \"\"\"\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 204\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [6217, 5688]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import configparser\n",
    "import json\n",
    "import csv\n",
    "from nilearn import plotting\n",
    "from enum import Enum, auto\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "# parse in config to see where to read the files in from\n",
    "\n",
    "DementiaStrings = [\"AD dem/FLD prior to AD dem\", \n",
    "                    \"AD dem w/depresss  not contribut\", \n",
    "                    \"AD dem w/depresss- not contribut\", \n",
    "                    \"AD dem w/Frontal lobe/demt at onset\", \n",
    "                    \"Incipient demt PTP\", \n",
    "                    \"AD dem w/oth (list B) not contrib\", \n",
    "                    \"AD dem distrubed social- prior\", \n",
    "                    \"Vascular Demt  primary\", \n",
    "                    \"AD dem Language dysf with\", \n",
    "                    \"AD dem distrubed social- with\", \n",
    "                    \"DAT\", \n",
    "                    \"DLBD- secondary\", \n",
    "                    \"DAT w/depresss not contribut\", \n",
    "                    \"Frontotemporal demt. prim\", \n",
    "                    \"AD dem Language dysf after\", \n",
    "                    \"AD dem w/CVD not contrib\", \n",
    "                    \"AD dem w/oth unusual features\", \n",
    "                    \"AD dem w/PDI after AD dem not contrib\", \n",
    "                    \"AD dem w/oth (list B) contribut\", \n",
    "                    \"AD dem cannot be primary\", \n",
    "                    \"AD dem Language dysf prior\", \n",
    "                    \"AD dem visuospatial- prior\", \n",
    "                    \"AD dem w/oth unusual features/demt on\", \n",
    "                    \"AD dem w/depresss- contribut\", \n",
    "                    \"AD dem w/CVD contribut\", \n",
    "                    \"AD dem visuospatial- with\", \n",
    "                    \"DLBD- primary\", \n",
    "                    \"Incipient Non-AD dem\", \n",
    "                    \"Dementia/PD- primary\", \n",
    "                    \"AD dem distrubed social- after\", \n",
    "                    \"Vascular Demt- secondary\", \n",
    "                    \"AD dem w/depresss  contribut\", \n",
    "                    \"AD Dementia\", \n",
    "                    \"AD dem w/PDI after AD dem contribut\", \n",
    "                    \"AD dem w/oth unusual feat/subs demt\", \n",
    "                    \"Vascular Demt- primary\"]\n",
    "NoDementiaStrings = [\"ProAph w/o dement\", \n",
    "                        \"Non AD dem- Other primary\", \n",
    "                        \"Cognitively normal\", \n",
    "                        \"No dementia\"]\n",
    "class Dementia(Enum):\n",
    "    DEMENTIA = \"Dementia\"\n",
    "    NO_DEMENTIA = \"No_Dementia\"\n",
    "    UNKNOWN = \"Unknown\"\n",
    "\n",
    "    @staticmethod\n",
    "    def from_str(string):\n",
    "        if string in DementiaStrings:\n",
    "            return Dementia.DEMENTIA\n",
    "        elif string in NoDementiaStrings:\n",
    "            return Dementia.NO_DEMENTIA\n",
    "        return Dementia.UNKNOWN\n",
    "\n",
    "\n",
    "patient_results = {}\n",
    "with open('../patient_diagnosis.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    headers = next(reader)\n",
    "    for row in reader:\n",
    "        rowObj = {}\n",
    "        for h, v in zip(headers, row):\n",
    "            rowObj[h] = v\n",
    "        if rowObj['Subject'] not in patient_results:\n",
    "            patient_results[rowObj['Subject']] = []\n",
    "        patient_results[rowObj['Subject']].append(rowObj)\n",
    "\n",
    "patient_id_to_results = {}\n",
    "for subject, subjectData in patient_results.items():\n",
    "    for data in subjectData:\n",
    "        if data['dx1'] is '':\n",
    "            continue\n",
    "        if subject not in patient_id_to_results:\n",
    "            patient_id_to_results[subject] = [Dementia.from_str(data['dx1'])]\n",
    "        else:\n",
    "            patient_id_to_results[subject].append(Dementia.from_str(data['dx1']))\n",
    "\n",
    "\n",
    "for subject, data in patient_id_to_results.items():\n",
    "    if len(data) is 1:\n",
    "        patient_id_to_results[subject] = data[0]\n",
    "    elif len(data) > 1 and len(set(data)) != 1:\n",
    "        if Dementia.DEMENTIA in data:\n",
    "            patient_id_to_results[subject] = Dementia.DEMENTIA\n",
    "        elif Dementia.NO_DEMENTIA in data:\n",
    "            patient_id_to_results[subject] = Dementia.NO_DEMENTIA\n",
    "        else:\n",
    "            patient_id_to_results[subject] = Dementia.UNKNOWN\n",
    "    else:\n",
    "        patient_id_to_results[subject] = data[0]\n",
    "\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('../config.ini')\n",
    "location = config['FILES']['Location']\n",
    "\n",
    "# find all the files in the folder\n",
    "raw_scan_ids = [x for x in os.listdir(location)]\n",
    "if '.DS_Store' in raw_scan_ids: raw_scan_ids.remove('.DS_Store')\n",
    "\n",
    "id_extractor_pattern = re.compile('(.*?)_')\n",
    "\n",
    "labelled_scan_ids = []\n",
    "\n",
    "for raw_scan_id in raw_scan_ids:\n",
    "    m = id_extractor_pattern.match(raw_scan_id)\n",
    "    scan_id = m.group(1)\n",
    "    if scan_id not in patient_id_to_results:\n",
    "        continue\n",
    "    elif patient_id_to_results[scan_id] is Dementia.UNKNOWN:\n",
    "        continue\n",
    "    labelled_scan_ids.append((scan_id, raw_scan_id))\n",
    "\n",
    "FLAG = \"T1w\" #or T2w\n",
    "mri_extractor_pattern = re.compile('anat[0-9]*')\n",
    "\n",
    "if FLAG is \"T1w\":\n",
    "    mri_type_pattern = re.compile(\".*T1w.nii.gz\")\n",
    "else:\n",
    "    mri_type_pattern = re.compile(\".*T2w.nii.gz\")\n",
    "\n",
    "paths = {}\n",
    "\n",
    "\n",
    "for scan_id, raw_scan_id in labelled_scan_ids:\n",
    "    raw_scan_id = location + \"/\" + raw_scan_id\n",
    "    scans = [x for x in os.listdir(raw_scan_id) if mri_extractor_pattern.match(x)]\n",
    "    scanLocations = []\n",
    "    for scan in scans:\n",
    "        scan_folder = raw_scan_id + \"/\" + scan\n",
    "        scanTypeMatch = [x for x in os.listdir(scan_folder) if mri_type_pattern.match(x)]\n",
    "        if scanTypeMatch:\n",
    "            scanLocations.append(scan_folder + \"/\" + scanTypeMatch[0])\n",
    "    if scan_id not in paths:\n",
    "        paths[scan_id] = []\n",
    "    paths[scan_id].append(scanLocations)\n",
    "print(paths)\n",
    "    \n",
    "# so the paths are ordered in a list like array, and idea is to take each index of list or\n",
    "# each file and feed .nii file to sci kit learn somehow. \n",
    "\n",
    "#after including plotting, attempt to plot the image? \n",
    "\n",
    "#attempting to use niilearn\n",
    "#from nibabel.testing import data_path \n",
    "#path it gave me was :'/Users/justinchang/anaconda3/lib/python3.6/site-packages/nibabel/tests/data/test.nii'\n",
    "#dont quite understand why i cant open it that way... \n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from nilearn import image\n",
    "\n",
    "# Let us use a Nifti file that is shipped with nilearn\n",
    "from nilearn.datasets import MNI152_FILE_PATH\n",
    "\n",
    "# Note that the variable MNI152_FILE_PATH is just a path to a Nifti file\n",
    "# print('Path to MNI152 template: %r' % MNI152_FILE_PATH)\n",
    "# %matplotlib inline\n",
    "# plotting.plot_img(MNI152_FILE_PATH)\n",
    "\n",
    "# plotting.plot_img(\"../files/OAS30001_MR_d0129/anat4/sub-OAS30001_sess-d0129_T2w.nii.gz\")\n",
    "# plotting.plot_glass_brain(\"../files/OAS30001_MR_d0129/anat4/sub-OAS30001_sess-d0129_T2w.nii.gz\")  \n",
    "#example_filename = os.path.join(data_path, 'test.nii')\n",
    "#img = nib.load(example_filename)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "#from nibabel.testing import paths\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Decoding with ANOVA + SVM: face vs house in the Haxby dataset\n",
    "===============================================================\n",
    "\n",
    "This example does a simple but efficient decoding on the Haxby dataset:\n",
    "using a feature selection, followed by an SVM.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#############################################################################\n",
    "# Retrieve the files of the Haxby dataset\n",
    "# ----------------------------------------\n",
    "#from nilearn import datasets\n",
    "\n",
    "# By default 2nd subject will be fetched\n",
    "from nilearn import datasets\n",
    "haxby_dataset = datasets.fetch_haxby()\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel='linear', C=1.)\n",
    "\n",
    "#example_filename = \"../files/OAS30001_MR_d0129/anat4/test.nii\"#paths[\"OAS30001\"] #feels bad cant evn hardcode a single path from the paths dict LOL\n",
    "import nibabel as nib \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for personId, MRISession in paths.items():\n",
    "#     print('asdf')\n",
    "#     for MRI in sorted(MRISession):\n",
    "#         for scan in sorted(MRI):\n",
    "#             img = nib.load(scan)\n",
    "#             print('Mask nifti image (3D) is located at (Note this is definitely not a \"mast nifti image\", was playing around with img.get_data(): %s' % img.get_data())\n",
    "#             print(scan) #this is the actual location of the data\n",
    "#         pass\n",
    "\n",
    "# print basic information on the dataset\n",
    "\n",
    "\n",
    "#svc.fit(data[:-10], labels[:-10])   \n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load target information as string and give a numerical identifier to each\n",
    "\n",
    "\n",
    "behavioral = pd.read_csv(\"../patient_diagnosis_processed.csv\")\n",
    "conditions = behavioral['dx1']\n",
    "\n",
    "# Restrict the analysis to faces and places\n",
    "condition_mask = behavioral['dx1'].isin(['Dementia','NoDementia'])\n",
    "conditions = conditions[condition_mask]\n",
    "\n",
    "# We now have 4 conditions\n",
    "print(conditions.unique())\n",
    "session = behavioral[condition_mask]\n",
    "\n",
    "\n",
    "\n",
    "from nilearn.input_data import NiftiMasker\n",
    "\n",
    "#file_name is a nii.gz\n",
    "# mask_filename = \"../files/OAS30001_MR_d0129/anat4/sub-OAS30001_sess-d0129_T2w.nii.gz\"\n",
    "\n",
    "# For decoding, standardizing is often very important\n",
    "# note that we are also smoothing the data\n",
    "masker = NiftiMasker(mask_img=haxby_dataset.mask, smoothing_fwhm=4,\n",
    "                     standardize=True, memory=\"nilearn_cache\", memory_level=1)\n",
    "\n",
    "#hard coded a filename for now\n",
    "func_filename = \"../files/OAS30001_MR_d0129/func1/sub-OAS30001_sess-d0129_task-rest_run-01_bold.nii.gz\"\n",
    "\n",
    "X = masker.fit_transform(func_filename)\n",
    "# Apply our condition_mask  \n",
    "\n",
    "X = X[condition_mask]\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel='linear')\n",
    "\n",
    "# Define the dimension reduction to be used.\n",
    "# Here we use a classical univariate feature selection based on F-test,\n",
    "# namely Anova. When doing full-brain analysis, it is better to use\n",
    "# SelectPercentile, keeping 5% of voxels\n",
    "# (because it is independent of the resolution of the data).\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "feature_selection = SelectPercentile(f_classif, percentile=5)\n",
    "\n",
    "# We have our classifier (SVC), our feature selection (SelectPercentile),and now,\n",
    "# we can plug them together in a *pipeline* that performs the two operations\n",
    "# successively:\n",
    "from sklearn.pipeline import Pipeline\n",
    "anova_svc = Pipeline([('anova', feature_selection), ('svc', svc)])\n",
    "\n",
    "#fit decoder, predict\n",
    "print(conditions.shape)\n",
    "anova_svc.fit(X, conditions)\n",
    "y_pred = anova_svc.predict(X)\n",
    "\n",
    "\n",
    "#obtaining score / accuracy via cross validation \n",
    "\n",
    "from sklearn.cross_validation import LeaveOneLabelOut, cross_val_score\n",
    "\n",
    "# Define the cross-validation scheme used for validation.\n",
    "# Here we use a LeaveOneLabelOut cross-validation on the session label\n",
    "# which corresponds to a leave-one-session-out\n",
    "cv = LeaveOneLabelOut(session['memory'])\n",
    "\n",
    "# Compute the prediction accuracy for the different folds (i.e. session)\n",
    "cv_scores = cross_val_score(anova_svc, X, conditions, cv=cv)\n",
    "\n",
    "# Return the corresponding mean prediction accuracy\n",
    "classification_accuracy = cv_scores.mean()\n",
    "\n",
    "# Print the results\n",
    "print(\"Classification accuracy: %.4f / Chance level: %f\" %\n",
    "      (classification_accuracy, 1. / len(conditions.unique())))\n",
    "# Classification accuracy:   / Chance level: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
