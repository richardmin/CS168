{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Voxel-Based Morphometry on Oasis dataset\n",
    "========================================\n",
    "\n",
    "This example uses Voxel-Based Morphometry (VBM) to study the relationship\n",
    "between aging and gray matter density.\n",
    "\n",
    "The data come from the `OASIS <http://www.oasis-brains.org/>`_ project.\n",
    "If you use it, you need to agree with the data usage agreement available\n",
    "on the website.\n",
    "\n",
    "It has been run through a standard VBM pipeline (using SPM8 and\n",
    "NewSegment) to create VBM maps, which we study here.\n",
    "\n",
    "Predictive modeling analysis: VBM bio-markers of aging?\n",
    "--------------------------------------------------------\n",
    "\n",
    "We run a standard SVM-ANOVA nilearn pipeline to predict age from the VBM\n",
    "data. We use only 100 subjects from the OASIS dataset to limit the memory\n",
    "usage.\n",
    "\n",
    "Note that for an actual predictive modeling study of aging, the study\n",
    "should be ran on the full set of subjects. Also, parameters such as the\n",
    "smoothing applied to the data and the number of features selected by the\n",
    "Anova step should be set by nested cross-validation, as they impact\n",
    "significantly the prediction score.\n",
    "\n",
    "Brain mapping with mass univariate\n",
    "-----------------------------------\n",
    "\n",
    "SVM weights are very noisy, partly because heavy smoothing is detrimental\n",
    "for the prediction here. A standard analysis using mass-univariate GLM\n",
    "(here permuted to have exact correction for multiple comparisons) gives a\n",
    "much clearer view of the important regions.\n",
    "\n",
    "____\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Authors: Elvis Dhomatob, <elvis.dohmatob@inria.fr>, Apr. 2014\n",
    "#          Virgile Fritsch, <virgile.fritsch@inria.fr>, Apr 2014\n",
    "#          Gael Varoquaux, Apr 2014\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn import datasets\n",
    "from nilearn.input_data import NiftiMasker\n",
    "\n",
    "n_subjects = 20 # more subjects requires more memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# import warnings\n",
    "import os\n",
    "from scipy import ndimage\n",
    "from sklearn.datasets.base import Bunch\n",
    "\n",
    "import base64\n",
    "import collections\n",
    "import contextlib\n",
    "import fnmatch\n",
    "import hashlib\n",
    "import shutil\n",
    "import time\n",
    "import sys\n",
    "import tarfile\n",
    "import warnings\n",
    "import zipfile\n",
    "\n",
    "import sys\n",
    "import hashlib\n",
    "\n",
    "from distutils.version import LooseVersion\n",
    "\n",
    "import nibabel\n",
    "\n",
    "\n",
    "if sys.version_info[0] == 3:\n",
    "    import pickle\n",
    "    import io\n",
    "    import urllib\n",
    "\n",
    "    _basestring = str\n",
    "    cPickle = pickle\n",
    "    StringIO = io.StringIO\n",
    "    BytesIO = io.BytesIO\n",
    "    _urllib = urllib\n",
    "    izip = zip\n",
    "\n",
    "    def md5_hash(string):\n",
    "        m = hashlib.md5()\n",
    "        m.update(string.encode('utf-8'))\n",
    "        return m.hexdigest()\n",
    "else:\n",
    "    import cPickle\n",
    "    import StringIO\n",
    "    import urllib\n",
    "    import urllib2\n",
    "    import urlparse\n",
    "    import types\n",
    "    import itertools\n",
    "\n",
    "    _basestring = basestring\n",
    "    cPickle = cPickle\n",
    "    StringIO = BytesIO = StringIO.StringIO\n",
    "    izip = itertools.izip\n",
    "\n",
    "    class _module_lookup(object):\n",
    "        modules = [urlparse, urllib2, urllib]\n",
    "\n",
    "        def __getattr__(self, name):\n",
    "            for module in self.modules:\n",
    "                if hasattr(module, name):\n",
    "                    attr = getattr(module, name)\n",
    "                    if not isinstance(attr, types.ModuleType):\n",
    "                        return attr\n",
    "            raise NotImplementedError(\n",
    "                'This function has not been imported properly')\n",
    "\n",
    "    module_lookup = _module_lookup()\n",
    "\n",
    "    class _urllib():\n",
    "        request = module_lookup\n",
    "        error = module_lookup\n",
    "        parse = module_lookup\n",
    "\n",
    "    def md5_hash(string):\n",
    "        m = hashlib.md5()\n",
    "        m.update(string)\n",
    "        return m.hexdigest()\n",
    "\n",
    "\n",
    "if LooseVersion(nibabel.__version__) >= LooseVersion('2.0.0'):\n",
    "    def get_affine(img):\n",
    "        return img.affine\n",
    "\n",
    "    def get_header(img):\n",
    "        return img.header\n",
    "else:\n",
    "    def get_affine(img):\n",
    "        return img.get_affine()\n",
    "\n",
    "    def get_header(img):\n",
    "        return img.get_header()\n",
    "\n",
    "\n",
    "def _format_time(t):\n",
    "    if t > 60:\n",
    "        return \"%4.1fmin\" % (t / 60.)\n",
    "    else:\n",
    "        return \" %5.1fs\" % (t)\n",
    "\n",
    "\n",
    "def _md5_sum_file(path):\n",
    "    \"\"\" Calculates the MD5 sum of a file.\n",
    "    \"\"\"\n",
    "    with open(path, 'rb') as f:\n",
    "        m = hashlib.md5()\n",
    "        while True:\n",
    "            data = f.read(8192)\n",
    "            if not data:\n",
    "                break\n",
    "            m.update(data)\n",
    "    return m.hexdigest()\n",
    "\n",
    "\n",
    "def _read_md5_sum_file(path):\n",
    "    \"\"\" Reads a MD5 checksum file and returns hashes as a dictionary.\n",
    "    \"\"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        hashes = {}\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            h, name = line.rstrip().split('  ', 1)\n",
    "            hashes[name] = h\n",
    "    return hashes\n",
    "\n",
    "\n",
    "def readlinkabs(link):\n",
    "    \"\"\"\n",
    "    Return an absolute path for the destination\n",
    "    of a symlink\n",
    "    \"\"\"\n",
    "    path = os.readlink(link)\n",
    "    if os.path.isabs(path):\n",
    "        return path\n",
    "    return os.path.join(os.path.dirname(link), path)\n",
    "\n",
    "\n",
    "def _chunk_report_(bytes_so_far, total_size, initial_size, t0):\n",
    "    \"\"\"Show downloading percentage.\n",
    "    Parameters\n",
    "    ----------\n",
    "    bytes_so_far: int\n",
    "        Number of downloaded bytes\n",
    "    total_size: int\n",
    "        Total size of the file (may be 0/None, depending on download method).\n",
    "    t0: int\n",
    "        The time in seconds (as returned by time.time()) at which the\n",
    "        download was resumed / started.\n",
    "    initial_size: int\n",
    "        If resuming, indicate the initial size of the file.\n",
    "        If not resuming, set to zero.\n",
    "    \"\"\"\n",
    "\n",
    "    if not total_size:\n",
    "        sys.stderr.write(\"\\rDownloaded %d of ? bytes.\" % (bytes_so_far))\n",
    "\n",
    "    else:\n",
    "        # Estimate remaining download time\n",
    "        total_percent = float(bytes_so_far) / total_size\n",
    "\n",
    "        current_download_size = bytes_so_far - initial_size\n",
    "        bytes_remaining = total_size - bytes_so_far\n",
    "        dt = time.time() - t0\n",
    "        download_rate = current_download_size / max(1e-8, float(dt))\n",
    "        # Minimum rate of 0.01 bytes/s, to avoid dividing by zero.\n",
    "        time_remaining = bytes_remaining / max(0.01, download_rate)\n",
    "\n",
    "        # Trailing whitespace is to erase extra char when message length\n",
    "        # varies\n",
    "        sys.stderr.write(\n",
    "            \"\\rDownloaded %d of %d bytes (%.1f%%, %s remaining)\"\n",
    "            % (bytes_so_far, total_size, total_percent * 100,\n",
    "               _format_time(time_remaining)))\n",
    "\n",
    "\n",
    "def _chunk_read_(response, local_file, chunk_size=8192, report_hook=None,\n",
    "                 initial_size=0, total_size=None, verbose=1):\n",
    "    \"\"\"Download a file chunk by chunk and show advancement\n",
    "    Parameters\n",
    "    ----------\n",
    "    response: _urllib.response.addinfourl\n",
    "        Response to the download request in order to get file size\n",
    "    local_file: file\n",
    "        Hard disk file where data should be written\n",
    "    chunk_size: int, optional\n",
    "        Size of downloaded chunks. Default: 8192\n",
    "    report_hook: bool\n",
    "        Whether or not to show downloading advancement. Default: None\n",
    "    initial_size: int, optional\n",
    "        If resuming, indicate the initial size of the file\n",
    "    total_size: int, optional\n",
    "        Expected final size of download (None means it is unknown).\n",
    "    verbose: int, optional\n",
    "        verbosity level (0 means no message).\n",
    "    Returns\n",
    "    -------\n",
    "    data: string\n",
    "        The downloaded file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if total_size is None:\n",
    "            total_size = response.info().get('Content-Length').strip()\n",
    "        total_size = int(total_size) + initial_size\n",
    "    except Exception as e:\n",
    "        if verbose > 2:\n",
    "            print(\"Warning: total size could not be determined.\")\n",
    "            if verbose > 3:\n",
    "                print(\"Full stack trace: %s\" % e)\n",
    "        total_size = None\n",
    "    bytes_so_far = initial_size\n",
    "\n",
    "    t0 = time_last_display = time.time()\n",
    "    while True:\n",
    "        chunk = response.read(chunk_size)\n",
    "        bytes_so_far += len(chunk)\n",
    "        time_last_read = time.time()\n",
    "        if (report_hook and\n",
    "                # Refresh report every half second or when download is\n",
    "                # finished.\n",
    "                (time_last_read > time_last_display + 0.5 or not chunk)):\n",
    "            _chunk_report_(bytes_so_far,\n",
    "                           total_size, initial_size, t0)\n",
    "            time_last_display = time_last_read\n",
    "        if chunk:\n",
    "            local_file.write(chunk)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def get_data_dirs(data_dir=None):\n",
    "    \"\"\" Returns the directories in which nilearn looks for data.\n",
    "    This is typically useful for the end-user to check where the data is\n",
    "    downloaded and stored.\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_dir: string, optional\n",
    "        Path of the data directory. Used to force data storage in a specified\n",
    "        location. Default: None\n",
    "    Returns\n",
    "    -------\n",
    "    paths: list of strings\n",
    "        Paths of the dataset directories.\n",
    "    Notes\n",
    "    -----\n",
    "    This function retrieves the datasets directories using the following\n",
    "    priority :\n",
    "    1. defaults system paths\n",
    "    2. the keyword argument data_dir\n",
    "    3. the global environment variable NILEARN_SHARED_DATA\n",
    "    4. the user environment variable NILEARN_DATA\n",
    "    5. nilearn_data in the user home folder\n",
    "    \"\"\"\n",
    "    # We build an array of successive paths by priority\n",
    "    # The boolean indicates if it is a pre_dir: in that case, we won't add the\n",
    "    # dataset name to the path.\n",
    "    paths = []\n",
    "\n",
    "    # Check data_dir which force storage in a specific location\n",
    "    if data_dir is not None:\n",
    "        paths.extend(data_dir.split(os.pathsep))\n",
    "\n",
    "    # If data_dir has not been specified, then we crawl default locations\n",
    "    if data_dir is None:\n",
    "        global_data = os.getenv('NILEARN_SHARED_DATA')\n",
    "        if global_data is not None:\n",
    "            paths.extend(global_data.split(os.pathsep))\n",
    "\n",
    "        local_data = os.getenv('NILEARN_DATA')\n",
    "        if local_data is not None:\n",
    "            paths.extend(local_data.split(os.pathsep))\n",
    "\n",
    "        paths.append(os.path.expanduser('~/nilearn_data'))\n",
    "    return paths\n",
    "\n",
    "\n",
    "def _get_dataset_dir(dataset_name, data_dir=None, default_paths=None,\n",
    "                     verbose=1):\n",
    "    \"\"\" Create if necessary and returns data directory of given dataset.\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_name: string\n",
    "        The unique name of the dataset.\n",
    "    data_dir: string, optional\n",
    "        Path of the data directory. Used to force data storage in a specified\n",
    "        location. Default: None\n",
    "    default_paths: list of string, optional\n",
    "        Default system paths in which the dataset may already have been\n",
    "        installed by a third party software. They will be checked first.\n",
    "    verbose: int, optional\n",
    "        verbosity level (0 means no message).\n",
    "    Returns\n",
    "    -------\n",
    "    data_dir: string\n",
    "        Path of the given dataset directory.\n",
    "    Notes\n",
    "    -----\n",
    "    This function retrieves the datasets directory (or data directory) using\n",
    "    the following priority :\n",
    "    1. defaults system paths\n",
    "    2. the keyword argument data_dir\n",
    "    3. the global environment variable NILEARN_SHARED_DATA\n",
    "    4. the user environment variable NILEARN_DATA\n",
    "    5. nilearn_data in the user home folder\n",
    "    \"\"\"\n",
    "    paths = []\n",
    "    # Search possible data-specific system paths\n",
    "    if default_paths is not None:\n",
    "        for default_path in default_paths:\n",
    "            paths.extend([(d, True) for d in default_path.split(os.pathsep)])\n",
    "\n",
    "    paths.extend([(d, False) for d in get_data_dirs(data_dir=data_dir)])\n",
    "\n",
    "    if verbose > 2:\n",
    "        print('Dataset search paths: %s' % paths)\n",
    "\n",
    "    # Check if the dataset exists somewhere\n",
    "    for path, is_pre_dir in paths:\n",
    "        if not is_pre_dir:\n",
    "            path = os.path.join(path, dataset_name)\n",
    "        if os.path.islink(path):\n",
    "            # Resolve path\n",
    "            path = readlinkabs(path)\n",
    "        if os.path.exists(path) and os.path.isdir(path):\n",
    "            if verbose > 1:\n",
    "                print('\\nDataset found in %s\\n' % path)\n",
    "            return path\n",
    "\n",
    "    # If not, create a folder in the first writeable directory\n",
    "    errors = []\n",
    "    for (path, is_pre_dir) in paths:\n",
    "        if not is_pre_dir:\n",
    "            path = os.path.join(path, dataset_name)\n",
    "        if not os.path.exists(path):\n",
    "            try:\n",
    "                os.makedirs(path)\n",
    "                if verbose > 0:\n",
    "                    print('\\nDataset created in %s\\n' % path)\n",
    "                return path\n",
    "            except Exception as exc:\n",
    "                short_error_message = getattr(exc, 'strerror', str(exc))\n",
    "                errors.append('\\n -{0} ({1})'.format(\n",
    "                    path, short_error_message))\n",
    "\n",
    "    raise OSError('Nilearn tried to store the dataset in the following '\n",
    "                  'directories, but:' + ''.join(errors))\n",
    "\n",
    "\n",
    "def _uncompress_file(file_, delete_archive=True, verbose=1):\n",
    "    \"\"\"Uncompress files contained in a data_set.\n",
    "    Parameters\n",
    "    ----------\n",
    "    file: string\n",
    "        path of file to be uncompressed.\n",
    "    delete_archive: bool, optional\n",
    "        Wheteher or not to delete archive once it is uncompressed.\n",
    "        Default: True\n",
    "    verbose: int, optional\n",
    "        verbosity level (0 means no message).\n",
    "    Notes\n",
    "    -----\n",
    "    This handles zip, tar, gzip and bzip files only.\n",
    "    \"\"\"\n",
    "    if verbose > 0:\n",
    "        sys.stderr.write('Extracting data from %s...' % file_)\n",
    "    data_dir = os.path.dirname(file_)\n",
    "    # We first try to see if it is a zip file\n",
    "    try:\n",
    "        filename, ext = os.path.splitext(file_)\n",
    "        with open(file_, \"rb\") as fd:\n",
    "            header = fd.read(4)\n",
    "        processed = False\n",
    "        if zipfile.is_zipfile(file_):\n",
    "            z = zipfile.ZipFile(file_)\n",
    "            z.extractall(path=data_dir)\n",
    "            z.close()\n",
    "            if delete_archive:\n",
    "                os.remove(file_)\n",
    "            file_ = filename\n",
    "            processed = True\n",
    "        elif ext == '.gz' or header.startswith(b'\\x1f\\x8b'):\n",
    "            import gzip\n",
    "            gz = gzip.open(file_)\n",
    "            if ext == '.tgz':\n",
    "                filename = filename + '.tar'\n",
    "            out = open(filename, 'wb')\n",
    "            shutil.copyfileobj(gz, out, 8192)\n",
    "            gz.close()\n",
    "            out.close()\n",
    "            # If file is .tar.gz, this will be handle in the next case\n",
    "            if delete_archive:\n",
    "                os.remove(file_)\n",
    "            file_ = filename\n",
    "            processed = True\n",
    "        if os.path.isfile(file_) and tarfile.is_tarfile(file_):\n",
    "            with contextlib.closing(tarfile.open(file_, \"r\")) as tar:\n",
    "                tar.extractall(path=data_dir)\n",
    "            if delete_archive:\n",
    "                os.remove(file_)\n",
    "            processed = True\n",
    "        if not processed:\n",
    "            raise IOError(\n",
    "                    \"[Uncompress] unknown archive file format: %s\" % file_)\n",
    "\n",
    "        if verbose > 0:\n",
    "            sys.stderr.write('.. done.\\n')\n",
    "    except Exception as e:\n",
    "        if verbose > 0:\n",
    "            print('Error uncompressing file: %s' % e)\n",
    "        raise\n",
    "\n",
    "\n",
    "def _filter_column(array, col, criteria):\n",
    "    \"\"\" Return index array matching criteria\n",
    "    Parameters\n",
    "    ----------\n",
    "    array: numpy array with columns\n",
    "        Array in which data will be filtered\n",
    "    col: string\n",
    "        Name of the column\n",
    "    criteria: integer (or float), pair of integers, string or list of these\n",
    "        if integer, select elements in column matching integer\n",
    "        if a tuple, select elements between the limits given by the tuple\n",
    "        if a string, select elements that match the string\n",
    "    \"\"\"\n",
    "    # Raise an error if the column does not exist. This is the only way to\n",
    "    # test it across all possible types (pandas, recarray...)\n",
    "    try:\n",
    "        array[col]\n",
    "    except:\n",
    "        raise KeyError('Filtering criterion %s does not exist' % col)\n",
    "\n",
    "    if (not isinstance(criteria, _basestring) and\n",
    "        not isinstance(criteria, bytes) and\n",
    "        not isinstance(criteria, tuple) and\n",
    "            isinstance(criteria, collections.Iterable)):\n",
    "        filter = np.zeros(array.shape[0], dtype=np.bool)\n",
    "        for criterion in criteria:\n",
    "            filter = np.logical_or(filter,\n",
    "                                   _filter_column(array, col, criterion))\n",
    "        return filter\n",
    "\n",
    "    if isinstance(criteria, tuple):\n",
    "        if len(criteria) != 2:\n",
    "            raise ValueError(\"An interval must have 2 values\")\n",
    "        if criteria[0] is None:\n",
    "            return array[col] <= criteria[1]\n",
    "        if criteria[1] is None:\n",
    "            return array[col] >= criteria[0]\n",
    "        filter = array[col] <= criteria[1]\n",
    "        return np.logical_and(filter, array[col] >= criteria[0])\n",
    "\n",
    "    # Handle strings with different encodings\n",
    "    if isinstance(criteria, (_basestring, bytes)):\n",
    "        criteria = np.array(criteria).astype(array[col].dtype)\n",
    "\n",
    "    return array[col] == criteria\n",
    "\n",
    "\n",
    "def _filter_columns(array, filters, combination='and'):\n",
    "    \"\"\" Return indices of recarray entries that match criteria.\n",
    "    Parameters\n",
    "    ----------\n",
    "    array: numpy array with columns\n",
    "        Array in which data will be filtered\n",
    "    filters: list of criteria\n",
    "        See _filter_column\n",
    "    combination: string, optional\n",
    "        String describing the combination operator. Possible values are \"and\"\n",
    "        and \"or\".\n",
    "    \"\"\"\n",
    "    if combination == 'and':\n",
    "        fcomb = np.logical_and\n",
    "        mask = np.ones(array.shape[0], dtype=np.bool)\n",
    "    elif combination == 'or':\n",
    "        fcomb = np.logical_or\n",
    "        mask = np.zeros(array.shape[0], dtype=np.bool)\n",
    "    else:\n",
    "        raise ValueError('Combination mode not known: %s' % combination)\n",
    "\n",
    "    for column in filters:\n",
    "        mask = fcomb(mask, _filter_column(array, column, filters[column]))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def _fetch_file(url, data_dir, resume=True, overwrite=False,\n",
    "                md5sum=None, username=None, password=None, handlers=[],\n",
    "                verbose=1):\n",
    "    \"\"\"Load requested file, downloading it if needed or requested.\n",
    "    Parameters\n",
    "    ----------\n",
    "    url: string\n",
    "        Contains the url of the file to be downloaded.\n",
    "    data_dir: string\n",
    "        Path of the data directory. Used for data storage in the specified\n",
    "        location.\n",
    "    resume: bool, optional\n",
    "        If true, try to resume partially downloaded files\n",
    "    overwrite: bool, optional\n",
    "        If true and file already exists, delete it.\n",
    "    md5sum: string, optional\n",
    "        MD5 sum of the file. Checked if download of the file is required\n",
    "    username: string, optional\n",
    "        Username used for basic HTTP authentication\n",
    "    password: string, optional\n",
    "        Password used for basic HTTP authentication\n",
    "    handlers: list of BaseHandler, optional\n",
    "        urllib handlers passed to urllib.request.build_opener. Used by\n",
    "        advanced users to customize request handling.\n",
    "    verbose: int, optional\n",
    "        verbosity level (0 means no message).\n",
    "    Returns\n",
    "    -------\n",
    "    files: string\n",
    "        Absolute path of downloaded file.\n",
    "    Notes\n",
    "    -----\n",
    "    If, for any reason, the download procedure fails, all downloaded files are\n",
    "    removed.\n",
    "    \"\"\"\n",
    "    # Determine data path\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "\n",
    "    # Determine filename using URL\n",
    "    parse = _urllib.parse.urlparse(url)\n",
    "    file_name = os.path.basename(parse.path)\n",
    "    if file_name == '':\n",
    "        file_name = md5_hash(parse.path)\n",
    "\n",
    "    temp_file_name = file_name + \".part\"\n",
    "    full_name = os.path.join(data_dir, file_name)\n",
    "    temp_full_name = os.path.join(data_dir, temp_file_name)\n",
    "    if os.path.exists(full_name):\n",
    "        if overwrite:\n",
    "            os.remove(full_name)\n",
    "        else:\n",
    "            return full_name\n",
    "    if os.path.exists(temp_full_name):\n",
    "        if overwrite:\n",
    "            os.remove(temp_full_name)\n",
    "    t0 = time.time()\n",
    "    local_file = None\n",
    "    initial_size = 0\n",
    "\n",
    "    try:\n",
    "        # Download data\n",
    "        url_opener = _urllib.request.build_opener(*handlers)\n",
    "        request = _urllib.request.Request(url)\n",
    "        request.add_header('Connection', 'Keep-Alive')\n",
    "        if username is not None and password is not None:\n",
    "            if not url.startswith('https'):\n",
    "                raise ValueError(\n",
    "                    'Authentication was requested on a non  secured URL (%s).'\n",
    "                    'Request has been blocked for security reasons.' % url)\n",
    "            # Note: HTTPBasicAuthHandler is not fitted here because it relies\n",
    "            # on the fact that the server will return a 401 error with proper\n",
    "            # www-authentication header, which is not the case of most\n",
    "            # servers.\n",
    "            encoded_auth = base64.b64encode(\n",
    "                (username + ':' + password).encode())\n",
    "            request.add_header(b'Authorization', b'Basic ' + encoded_auth)\n",
    "        if verbose > 0:\n",
    "            displayed_url = url.split('?')[0] if verbose == 1 else url\n",
    "            print('Downloading data from %s ...' % displayed_url)\n",
    "        if resume and os.path.exists(temp_full_name):\n",
    "            # Download has been interrupted, we try to resume it.\n",
    "            local_file_size = os.path.getsize(temp_full_name)\n",
    "            # If the file exists, then only download the remainder\n",
    "            request.add_header(\"Range\", \"bytes=%s-\" % (local_file_size))\n",
    "            try:\n",
    "                data = url_opener.open(request)\n",
    "                content_range = data.info().get('Content-Range')\n",
    "                if (content_range is None or not content_range.startswith(\n",
    "                        'bytes %s-' % local_file_size)):\n",
    "                    raise IOError('Server does not support resuming')\n",
    "            except Exception:\n",
    "                # A wide number of errors can be raised here. HTTPError,\n",
    "                # URLError... I prefer to catch them all and rerun without\n",
    "                # resuming.\n",
    "                if verbose > 0:\n",
    "                    print('Resuming failed, try to download the whole file.')\n",
    "                return _fetch_file(\n",
    "                    url, data_dir, resume=False, overwrite=overwrite,\n",
    "                    md5sum=md5sum, username=username, password=password,\n",
    "                    handlers=handlers, verbose=verbose)\n",
    "            local_file = open(temp_full_name, \"ab\")\n",
    "            initial_size = local_file_size\n",
    "        else:\n",
    "            data = url_opener.open(request)\n",
    "            local_file = open(temp_full_name, \"wb\")\n",
    "        _chunk_read_(data, local_file, report_hook=(verbose > 0),\n",
    "                     initial_size=initial_size, verbose=verbose)\n",
    "        # temp file must be closed prior to the move\n",
    "        if not local_file.closed:\n",
    "            local_file.close()\n",
    "        shutil.move(temp_full_name, full_name)\n",
    "        dt = time.time() - t0\n",
    "        if verbose > 0:\n",
    "            # Complete the reporting hook\n",
    "            sys.stderr.write(' ...done. ({0:.0f} seconds, {1:.0f} min)\\n'\n",
    "                             .format(dt, dt // 60))\n",
    "    except (_urllib.error.HTTPError, _urllib.error.URLError) as e:\n",
    "        if 'Error while fetching' not in str(e):\n",
    "            # For some odd reason, the error message gets doubled up\n",
    "            #   (possibly from the re-raise), so only add extra info\n",
    "            #   if it's not already there.\n",
    "            e.reason = (\"%s| Error while fetching file %s; \"\n",
    "                          \"dataset fetching aborted.\" % (\n",
    "                            str(e.reason), file_name))\n",
    "        raise\n",
    "    finally:\n",
    "        if local_file is not None:\n",
    "            if not local_file.closed:\n",
    "                local_file.close()\n",
    "    if md5sum is not None:\n",
    "        if (_md5_sum_file(full_name) != md5sum):\n",
    "            raise ValueError(\"File %s checksum verification has failed.\"\n",
    "                             \" Dataset fetching aborted.\" % local_file)\n",
    "    return full_name\n",
    "\n",
    "\n",
    "def _get_dataset_descr(ds_name):\n",
    "#     module_path = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "#     fname = ds_name\n",
    "\n",
    "#     try:\n",
    "#         with open(os.path.join(module_path, 'description', fname + '.rst'),\n",
    "#                   'rb') as rst_file:\n",
    "#             descr = rst_file.read()\n",
    "#     except IOError:\n",
    "#         descr = ''\n",
    "\n",
    "#     if descr == '':\n",
    "#         print(\"Warning: Could not find dataset description.\")\n",
    "\n",
    "#     return descr\n",
    "    return ''\n",
    "\n",
    "\n",
    "def movetree(src, dst):\n",
    "    \"\"\"Move an entire tree to another directory. Any existing file is\n",
    "    overwritten\"\"\"\n",
    "    names = os.listdir(src)\n",
    "\n",
    "    # Create destination dir if it does not exist\n",
    "    if not os.path.exists(dst):\n",
    "        os.makedirs(dst)\n",
    "    errors = []\n",
    "\n",
    "    for name in names:\n",
    "        srcname = os.path.join(src, name)\n",
    "        dstname = os.path.join(dst, name)\n",
    "        try:\n",
    "            if os.path.isdir(srcname) and os.path.isdir(dstname):\n",
    "                movetree(srcname, dstname)\n",
    "                os.rmdir(srcname)\n",
    "            else:\n",
    "                shutil.move(srcname, dstname)\n",
    "        except (IOError, os.error) as why:\n",
    "            errors.append((srcname, dstname, str(why)))\n",
    "        # catch the Error from the recursive movetree so that we can\n",
    "        # continue with other files\n",
    "        except Exception as err:\n",
    "            errors.extend(err.args[0])\n",
    "    if errors:\n",
    "        raise Exception(errors)\n",
    "\n",
    "\n",
    "def _fetch_files(data_dir, files, resume=True, mock=False, verbose=1):\n",
    "    \"\"\"Load requested dataset, downloading it if needed or requested.\n",
    "    This function retrieves files from the hard drive or download them from\n",
    "    the given urls. Note to developpers: All the files will be first\n",
    "    downloaded in a sandbox and, if everything goes well, they will be moved\n",
    "    into the folder of the dataset. This prevents corrupting previously\n",
    "    downloaded data. In case of a big dataset, do not hesitate to make several\n",
    "    calls if needed.\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_dir: string\n",
    "        Path of the data directory. Used for data storage in a specified\n",
    "        location.\n",
    "    files: list of (string, string, dict)\n",
    "        List of files and their corresponding url with dictionary that contains\n",
    "        options regarding the files. Eg. (file_path, url, opt). If a file_path\n",
    "        is not found in data_dir, as in data_dir/file_path the download will\n",
    "        be immediately cancelled and any downloaded files will be deleted.\n",
    "        Options supported are:\n",
    "            * 'move' if renaming the file or moving it to a subfolder is needed\n",
    "            * 'uncompress' to indicate that the file is an archive\n",
    "            * 'md5sum' to check the md5 sum of the file\n",
    "            * 'overwrite' if the file should be re-downloaded even if it exists\n",
    "    resume: bool, optional\n",
    "        If true, try resuming download if possible\n",
    "    mock: boolean, optional\n",
    "        If true, create empty files if the file cannot be downloaded. Test use\n",
    "        only.\n",
    "    verbose: int, optional\n",
    "        verbosity level (0 means no message).\n",
    "    Returns\n",
    "    -------\n",
    "    files: list of string\n",
    "        Absolute paths of downloaded files on disk\n",
    "    \"\"\"\n",
    "    # There are two working directories here:\n",
    "    # - data_dir is the destination directory of the dataset\n",
    "    # - temp_dir is a temporary directory dedicated to this fetching call. All\n",
    "    #   files that must be downloaded will be in this directory. If a corrupted\n",
    "    #   file is found, or a file is missing, this working directory will be\n",
    "    #   deleted.\n",
    "    files = list(files)\n",
    "    files_pickle = cPickle.dumps([(file_, url) for file_, url, _ in files])\n",
    "    files_md5 = hashlib.md5(files_pickle).hexdigest()\n",
    "    temp_dir = os.path.join(data_dir, files_md5)\n",
    "\n",
    "    # Create destination dir\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "\n",
    "    # Abortion flag, in case of error\n",
    "    abort = None\n",
    "\n",
    "    files_ = []\n",
    "    for file_, url, opts in files:\n",
    "        # 3 possibilities:\n",
    "        # - the file exists in data_dir, nothing to do.\n",
    "        # - the file does not exists: we download it in temp_dir\n",
    "        # - the file exists in temp_dir: this can happen if an archive has been\n",
    "        #   downloaded. There is nothing to do\n",
    "\n",
    "        # Target file in the data_dir\n",
    "        target_file = os.path.join(data_dir, file_)\n",
    "        # Target file in temp dir\n",
    "        temp_target_file = os.path.join(temp_dir, file_)\n",
    "        # Whether to keep existing files\n",
    "        overwrite = opts.get('overwrite', False)\n",
    "        if (abort is None and (overwrite or (not os.path.exists(target_file) and not\n",
    "                os.path.exists(temp_target_file)))):\n",
    "\n",
    "            # We may be in a global read-only repository. If so, we cannot\n",
    "            # download files.\n",
    "            if not os.access(data_dir, os.W_OK):\n",
    "                raise ValueError('Dataset files are missing but dataset'\n",
    "                                 ' repository is read-only. Contact your data'\n",
    "                                 ' administrator to solve the problem')\n",
    "\n",
    "            if not os.path.exists(temp_dir):\n",
    "                os.mkdir(temp_dir)\n",
    "            md5sum = opts.get('md5sum', None)\n",
    "\n",
    "            dl_file = _fetch_file(url, temp_dir, resume=resume,\n",
    "                                  verbose=verbose, md5sum=md5sum,\n",
    "                                  username=opts.get('username', None),\n",
    "                                  password=opts.get('password', None),\n",
    "                                  handlers=opts.get('handlers', []),\n",
    "                                  overwrite=overwrite)\n",
    "            if 'move' in opts:\n",
    "                # XXX: here, move is supposed to be a dir, it can be a name\n",
    "                move = os.path.join(temp_dir, opts['move'])\n",
    "                move_dir = os.path.dirname(move)\n",
    "                if not os.path.exists(move_dir):\n",
    "                    os.makedirs(move_dir)\n",
    "                shutil.move(dl_file, move)\n",
    "                dl_file = move\n",
    "            if 'uncompress' in opts:\n",
    "                try:\n",
    "                    if not mock or os.path.getsize(dl_file) != 0:\n",
    "                        _uncompress_file(dl_file, verbose=verbose)\n",
    "                    else:\n",
    "                        os.remove(dl_file)\n",
    "                except Exception as e:\n",
    "                    abort = str(e)\n",
    "\n",
    "        if (abort is None and not os.path.exists(target_file) and not\n",
    "                os.path.exists(temp_target_file)):\n",
    "            if not mock:\n",
    "                warnings.warn('An error occured while fetching %s' % file_)\n",
    "                abort = (\"Dataset has been downloaded but requested file was \"\n",
    "                         \"not provided:\\nURL: %s\\n\"\n",
    "                         \"Target file: %s\\nDownloaded: %s\" %\n",
    "                         (url, target_file, dl_file))\n",
    "            else:\n",
    "                if not os.path.exists(os.path.dirname(temp_target_file)):\n",
    "                    os.makedirs(os.path.dirname(temp_target_file))\n",
    "                open(temp_target_file, 'w').close()\n",
    "        if abort is not None:\n",
    "            if os.path.exists(temp_dir):\n",
    "                shutil.rmtree(temp_dir)\n",
    "            raise IOError('Fetching aborted: ' + abort)\n",
    "        files_.append(target_file)\n",
    "    # If needed, move files from temps directory to final directory.\n",
    "    if os.path.exists(temp_dir):\n",
    "        # XXX We could only moved the files requested\n",
    "        # XXX Movetree can go wrong\n",
    "        movetree(temp_dir, data_dir)\n",
    "        shutil.rmtree(temp_dir)\n",
    "    return files_\n",
    "\n",
    "\n",
    "def _tree(path, pattern=None, dictionary=False):\n",
    "    \"\"\" Return a directory tree under the form of a dictionaries and list\n",
    "    Parameters:\n",
    "    -----------\n",
    "    path: string\n",
    "        Path browsed\n",
    "    pattern: string, optional\n",
    "        Pattern used to filter files (see fnmatch)\n",
    "    dictionary: boolean, optional\n",
    "        If True, the function will return a dict instead of a list\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    dirs = [] if not dictionary else {}\n",
    "    for file_ in os.listdir(path):\n",
    "        file_path = os.path.join(path, file_)\n",
    "        if os.path.isdir(file_path):\n",
    "            if not dictionary:\n",
    "                dirs.append((file_, _tree(file_path, pattern)))\n",
    "            else:\n",
    "                dirs[file_] = _tree(file_path, pattern)\n",
    "        else:\n",
    "            if pattern is None or fnmatch.fnmatch(file_, pattern):\n",
    "                files.append(file_path)\n",
    "    files = sorted(files)\n",
    "    if not dictionary:\n",
    "        return sorted(dirs) + files\n",
    "    if len(dirs) == 0:\n",
    "        return files\n",
    "    if len(files) > 0:\n",
    "        dirs['.'] = files\n",
    "    return dirs\n",
    "\n",
    "\n",
    "def fetch_oasis2_vbm(n_subjects=None, dartel_version=False, data_dir=None,\n",
    "                    url=None, resume=True, verbose=1):\n",
    "    \"\"\"Download and load Oasis \"cross-sectional MRI\" dataset (416 subjects).\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_subjects: int, optional\n",
    "        The number of subjects to load. If None is given, all the\n",
    "        subjects are used.\n",
    "    dartel_version: boolean,\n",
    "        Whether or not to use data normalized with DARTEL instead of standard\n",
    "        SPM8 normalization.\n",
    "    data_dir: string, optional\n",
    "        Path of the data directory. Used to force data storage in a specified\n",
    "        location. Default: None\n",
    "    url: string, optional\n",
    "        Override download URL. Used for test only (or if you setup a mirror of\n",
    "        the data).\n",
    "    resume: bool, optional\n",
    "        If true, try resuming download if possible\n",
    "    verbose: int, optional\n",
    "        verbosity level (0 means no message).\n",
    "    Returns\n",
    "    -------\n",
    "    data: Bunch\n",
    "        Dictionary-like object, the interest attributes are :\n",
    "        - 'gray_matter_maps': string list\n",
    "          Paths to nifti gray matter density probability maps\n",
    "        - 'white_matter_maps' string list\n",
    "          Paths to nifti white matter density probability maps\n",
    "        - 'ext_vars': np.recarray\n",
    "          Data from the .csv file with information about selected subjects\n",
    "        - 'data_usage_agreement': string\n",
    "          Path to the .txt file containing the data usage agreement.\n",
    "    References\n",
    "    ----------\n",
    "    [1] http://www.oasis-brains.org/\n",
    "    [2] Open Access Series of Imaging Studies (OASIS): Cross-sectional MRI\n",
    "        Data in Young, Middle Aged, Nondemented, and Demented Older Adults.\n",
    "        Marcus, D. S and al., 2007, Journal of Cognitive Neuroscience.\n",
    "    Notes\n",
    "    -----\n",
    "    In the DARTEL version, original Oasis data [1] have been preprocessed\n",
    "    with the following steps:\n",
    "      1. Dimension swapping (technically required for subsequent steps)\n",
    "      2. Brain Extraction\n",
    "      3. Segmentation with SPM8\n",
    "      4. Normalization using DARTEL algorithm\n",
    "      5. Modulation\n",
    "      6. Replacement of NaN values with 0 in gray/white matter density maps.\n",
    "      7. Resampling to reduce shape and make it correspond to the shape of\n",
    "         the non-DARTEL data (fetched with dartel_version=False).\n",
    "      8. Replacement of values < 1e-4 with zeros to reduce the file size.\n",
    "    In the non-DARTEL version, the following steps have been performed instead:\n",
    "      1. Dimension swapping (technically required for subsequent steps)\n",
    "      2. Brain Extraction\n",
    "      3. Segmentation and normalization to a template with SPM8\n",
    "      4. Modulation\n",
    "      5. Replacement of NaN values with 0 in gray/white matter density maps.\n",
    "    An archive containing the gray and white matter density probability maps\n",
    "    for the 416 available subjects is provided. Gross outliers are removed and\n",
    "    filtered by this data fetcher (DARTEL: 13 outliers; non-DARTEL: 1 outlier)\n",
    "    Externals variates (age, gender, estimated intracranial volume,\n",
    "    years of education, socioeconomic status, dementia score) are provided\n",
    "    in a CSV file that is a copy of the original Oasis CSV file. The current\n",
    "    downloader loads the CSV file and keeps only the lines corresponding to\n",
    "    the subjects that are actually demanded.\n",
    "    The Open Access Structural Imaging Series (OASIS) is a project\n",
    "    dedicated to making brain imaging data openly available to the public.\n",
    "    Using data available through the OASIS project requires agreeing with\n",
    "    the Data Usage Agreement that can be found at\n",
    "    http://www.oasis-brains.org/app/template/UsageAgreement.vm\n",
    "    \"\"\"\n",
    "    # check number of subjects\n",
    "    if n_subjects is None:\n",
    "        n_subjects = 403 if dartel_version else 415\n",
    "    if dartel_version:  # DARTEL version has 13 identified outliers\n",
    "        if n_subjects > 403:\n",
    "            warnings.warn('Only 403 subjects are available in the '\n",
    "                          'DARTEL-normalized version of the dataset. '\n",
    "                          'All of them will be used instead of the wanted %d'\n",
    "                          % n_subjects)\n",
    "            n_subjects = 403\n",
    "    else:  # all subjects except one are available with non-DARTEL version\n",
    "        if n_subjects > 415:\n",
    "            warnings.warn('Only 415 subjects are available in the '\n",
    "                          'non-DARTEL-normalized version of the dataset. '\n",
    "                          'All of them will be used instead of the wanted %d'\n",
    "                          % n_subjects)\n",
    "            n_subjects = 415\n",
    "    if n_subjects < 1:\n",
    "        raise ValueError(\"Incorrect number of subjects (%d)\" % n_subjects)\n",
    "\n",
    "    # pick the archive corresponding to preprocessings type\n",
    "    if url is None:\n",
    "        if dartel_version:\n",
    "            url_images = ('https://www.nitrc.org/frs/download.php/'\n",
    "                          '6364/archive_dartel.tgz?i_agree=1&download_now=1')\n",
    "        else:\n",
    "            url_images = ('https://www.nitrc.org/frs/download.php/'\n",
    "                          '6359/archive.tgz?i_agree=1&download_now=1')\n",
    "        # covariates and license are in separate files on NITRC\n",
    "        url_csv = ('https://www.nitrc.org/frs/download.php/'\n",
    "                   '6348/oasis_cross-sectional.csv?i_agree=1&download_now=1')\n",
    "        url_dua = ('https://www.nitrc.org/frs/download.php/'\n",
    "                   '6349/data_usage_agreement.txt?i_agree=1&download_now=1')\n",
    "    else:  # local URL used in tests\n",
    "        url_csv = url + \"/oasis_cross-sectional.csv\"\n",
    "        url_dua = url + \"/data_usage_agreement.txt\"\n",
    "        if dartel_version:\n",
    "            url_images = url + \"/archive_dartel.tgz\"\n",
    "        else:\n",
    "            url_images = url + \"/archive.tgz\"\n",
    "\n",
    "    opts = {'uncompress': True}\n",
    "\n",
    "    # missing subjects create shifts in subjects ids\n",
    "    missing_subjects = [8, 24, 36, 48, 89, 93, 100, 118, 128, 149, 154,\n",
    "                        171, 172, 175, 187, 194, 196, 215, 219, 225, 242,\n",
    "                        245, 248, 251, 252, 257, 276, 297, 306, 320, 324,\n",
    "                        334, 347, 360, 364, 391, 393, 412, 414, 427, 436]\n",
    "\n",
    "    if dartel_version:\n",
    "        # DARTEL produces outliers that are hidden by nilearn API\n",
    "        removed_outliers = [27, 57, 66, 83, 122, 157, 222, 269, 282, 287,\n",
    "                            309, 428]\n",
    "        missing_subjects = sorted(missing_subjects + removed_outliers)\n",
    "        file_names_gm = [\n",
    "            (os.path.join(\n",
    "                    \"OAS2_%04d_MR1/RAW/\",\n",
    "                    \"mpr-1.nifti.img\")\n",
    "             % (s),\n",
    "             url_images, opts)\n",
    "            for s in range(1, 186) if s not in missing_subjects][:n_subjects]\n",
    "        file_names_wm = [\n",
    "            (os.path.join(\n",
    "                    \"OAS2_%04d_MR1/RAW/\",\n",
    "                    \"mpr-2.nifti.img\")\n",
    "             % (s),\n",
    "             url_images, opts)\n",
    "            for s in range(1, 186) if s not in missing_subjects]\n",
    "    else:\n",
    "        # only one gross outlier produced, hidden by nilearn API\n",
    "        removed_outliers = [390]\n",
    "        missing_subjects = sorted(missing_subjects + removed_outliers)\n",
    "        file_names_gm = [\n",
    "            (os.path.join(\n",
    "                    \"OAS2_%04d_MR1/RAW/\",\n",
    "                    \"mpr-1.nifti.img\")\n",
    "             % (s),\n",
    "             url_images, opts)\n",
    "            for s in range(1, 186) if s not in missing_subjects][:n_subjects]\n",
    "        file_names_wm = [\n",
    "            (os.path.join(\n",
    "                    \"OAS2_%04d_MR1/RAW/\",\n",
    "                    \"mpr-2.nifti.img\")\n",
    "             % (s),\n",
    "             url_images, opts)\n",
    "            for s in range(1, 186) if s not in missing_subjects]\n",
    "    file_names_extvars = [(\"oasis_cross-sectional.csv\", url_csv, {})]\n",
    "    file_names_dua = [(\"data_usage_agreement.txt\", url_dua, {})]\n",
    "    # restrict to user-specified number of subjects\n",
    "    file_names_gm = file_names_gm[:n_subjects]\n",
    "    file_names_wm = file_names_wm[:n_subjects]\n",
    "\n",
    "    file_names = (file_names_gm + file_names_wm +\n",
    "                  file_names_extvars + file_names_dua)\n",
    "    dataset_name = 'oasis2'\n",
    "    data_dir = _get_dataset_dir(dataset_name, data_dir=data_dir,\n",
    "                                verbose=verbose)\n",
    "    files = ['/Users/RichardMin/nilearn_data/oasis2/OAS2_0001_MR1/RAW/mpr-1.nifti.img', '/Users/RichardMin/nilearn_data/oasis2/OAS2_0002_MR1/RAW/mpr-1.nifti.img']\n",
    "\n",
    "    # Build Bunch\n",
    "    print(n_subjects)\n",
    "    gm_maps = files[:n_subjects]\n",
    "    wm_maps = files[n_subjects:(2 * n_subjects)]\n",
    "    ext_vars_file = \"/Users/RichardMin/nilearn_data/oasis2/oasis_longitudinal.csv\"\n",
    "    data_usage_agreement = files[-1]\n",
    "\n",
    "    # Keep CSV information only for selected subjects\n",
    "    csv_data = np.recfromcsv(ext_vars_file)\n",
    "    # Comparisons to recfromcsv data must be bytes.\n",
    "    actual_subjects_ids = [(\"OAS2\" +\n",
    "                            str.split(os.path.basename(x),\n",
    "                                      \"OAS2\")[1][:9]).encode()\n",
    "                           for x in gm_maps]\n",
    "    subject_mask = np.asarray([subject_id in actual_subjects_ids\n",
    "                               for subject_id in csv_data['id']])\n",
    "    csv_data = csv_data[subject_mask]\n",
    "    print(actual_subjects_ids)\n",
    "\n",
    "    fdescr = _get_dataset_descr(dataset_name)\n",
    "\n",
    "    return Bunch(\n",
    "        gray_matter_maps=gm_maps,\n",
    "        white_matter_maps=wm_maps,\n",
    "        ext_vars=csv_data,\n",
    "        data_usage_agreement=data_usage_agreement,\n",
    "description=fdescr)\n",
    "\n",
    "\n",
    "\n",
    "def fetch_oasis_vbm(n_subjects=None, dartel_version=True, data_dir=None,\n",
    "                    url=None, resume=True, verbose=1):\n",
    "    \"\"\"Download and load Oasis \"cross-sectional MRI\" dataset (416 subjects).\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_subjects: int, optional\n",
    "        The number of subjects to load. If None is given, all the\n",
    "        subjects are used.\n",
    "    dartel_version: boolean,\n",
    "        Whether or not to use data normalized with DARTEL instead of standard\n",
    "        SPM8 normalization.\n",
    "    data_dir: string, optional\n",
    "        Path of the data directory. Used to force data storage in a specified\n",
    "        location. Default: None\n",
    "    url: string, optional\n",
    "        Override download URL. Used for test only (or if you setup a mirror of\n",
    "        the data).\n",
    "    resume: bool, optional\n",
    "        If true, try resuming download if possible\n",
    "    verbose: int, optional\n",
    "        verbosity level (0 means no message).\n",
    "    Returns\n",
    "    -------\n",
    "    data: Bunch\n",
    "        Dictionary-like object, the interest attributes are :\n",
    "        - 'gray_matter_maps': string list\n",
    "          Paths to nifti gray matter density probability maps\n",
    "        - 'white_matter_maps' string list\n",
    "          Paths to nifti white matter density probability maps\n",
    "        - 'ext_vars': np.recarray\n",
    "          Data from the .csv file with information about selected subjects\n",
    "        - 'data_usage_agreement': string\n",
    "          Path to the .txt file containing the data usage agreement.\n",
    "    References\n",
    "    ----------\n",
    "    [1] http://www.oasis-brains.org/\n",
    "    [2] Open Access Series of Imaging Studies (OASIS): Cross-sectional MRI\n",
    "        Data in Young, Middle Aged, Nondemented, and Demented Older Adults.\n",
    "        Marcus, D. S and al., 2007, Journal of Cognitive Neuroscience.\n",
    "    Notes\n",
    "    -----\n",
    "    In the DARTEL version, original Oasis data [1] have been preprocessed\n",
    "    with the following steps:\n",
    "      1. Dimension swapping (technically required for subsequent steps)\n",
    "      2. Brain Extraction\n",
    "      3. Segmentation with SPM8\n",
    "      4. Normalization using DARTEL algorithm\n",
    "      5. Modulation\n",
    "      6. Replacement of NaN values with 0 in gray/white matter density maps.\n",
    "      7. Resampling to reduce shape and make it correspond to the shape of\n",
    "         the non-DARTEL data (fetched with dartel_version=False).\n",
    "      8. Replacement of values < 1e-4 with zeros to reduce the file size.\n",
    "    In the non-DARTEL version, the following steps have been performed instead:\n",
    "      1. Dimension swapping (technically required for subsequent steps)\n",
    "      2. Brain Extraction\n",
    "      3. Segmentation and normalization to a template with SPM8\n",
    "      4. Modulation\n",
    "      5. Replacement of NaN values with 0 in gray/white matter density maps.\n",
    "    An archive containing the gray and white matter density probability maps\n",
    "    for the 416 available subjects is provided. Gross outliers are removed and\n",
    "    filtered by this data fetcher (DARTEL: 13 outliers; non-DARTEL: 1 outlier)\n",
    "    Externals variates (age, gender, estimated intracranial volume,\n",
    "    years of education, socioeconomic status, dementia score) are provided\n",
    "    in a CSV file that is a copy of the original Oasis CSV file. The current\n",
    "    downloader loads the CSV file and keeps only the lines corresponding to\n",
    "    the subjects that are actually demanded.\n",
    "    The Open Access Structural Imaging Series (OASIS) is a project\n",
    "    dedicated to making brain imaging data openly available to the public.\n",
    "    Using data available through the OASIS project requires agreeing with\n",
    "    the Data Usage Agreement that can be found at\n",
    "    http://www.oasis-brains.org/app/template/UsageAgreement.vm\n",
    "    \"\"\"\n",
    "    # check number of subjects\n",
    "    if n_subjects is None:\n",
    "        n_subjects = 403 if dartel_version else 415\n",
    "    if dartel_version:  # DARTEL version has 13 identified outliers\n",
    "        if n_subjects > 403:\n",
    "            warnings.warn('Only 403 subjects are available in the '\n",
    "                          'DARTEL-normalized version of the dataset. '\n",
    "                          'All of them will be used instead of the wanted %d'\n",
    "                          % n_subjects)\n",
    "            n_subjects = 403\n",
    "    else:  # all subjects except one are available with non-DARTEL version\n",
    "        if n_subjects > 415:\n",
    "            warnings.warn('Only 415 subjects are available in the '\n",
    "                          'non-DARTEL-normalized version of the dataset. '\n",
    "                          'All of them will be used instead of the wanted %d'\n",
    "                          % n_subjects)\n",
    "            n_subjects = 415\n",
    "    if n_subjects < 1:\n",
    "        raise ValueError(\"Incorrect number of subjects (%d)\" % n_subjects)\n",
    "\n",
    "    # pick the archive corresponding to preprocessings type\n",
    "    if url is None:\n",
    "        if dartel_version:\n",
    "            url_images = ('https://www.nitrc.org/frs/download.php/'\n",
    "                          '6364/archive_dartel.tgz?i_agree=1&download_now=1')\n",
    "        else:\n",
    "            url_images = ('https://www.nitrc.org/frs/download.php/'\n",
    "                          '6359/archive.tgz?i_agree=1&download_now=1')\n",
    "        # covariates and license are in separate files on NITRC\n",
    "        url_csv = ('https://www.nitrc.org/frs/download.php/'\n",
    "                   '6348/oasis_cross-sectional.csv?i_agree=1&download_now=1')\n",
    "        url_dua = ('https://www.nitrc.org/frs/download.php/'\n",
    "                   '6349/data_usage_agreement.txt?i_agree=1&download_now=1')\n",
    "    else:  # local URL used in tests\n",
    "        url_csv = url + \"/oasis_cross-sectional.csv\"\n",
    "        url_dua = url + \"/data_usage_agreement.txt\"\n",
    "        if dartel_version:\n",
    "            url_images = url + \"/archive_dartel.tgz\"\n",
    "        else:\n",
    "            url_images = url + \"/archive.tgz\"\n",
    "\n",
    "    opts = {'uncompress': True}\n",
    "\n",
    "    # missing subjects create shifts in subjects ids\n",
    "    missing_subjects = [8, 24, 36, 48, 89, 93, 100, 118, 128, 149, 154,\n",
    "                        171, 172, 175, 187, 194, 196, 215, 219, 225, 242,\n",
    "                        245, 248, 251, 252, 257, 276, 297, 306, 320, 324,\n",
    "                        334, 347, 360, 364, 391, 393, 412, 414, 427, 436]\n",
    "\n",
    "    if dartel_version:\n",
    "        # DARTEL produces outliers that are hidden by nilearn API\n",
    "        removed_outliers = [27, 57, 66, 83, 122, 157, 222, 269, 282, 287,\n",
    "                            309, 428]\n",
    "        missing_subjects = sorted(missing_subjects + removed_outliers)\n",
    "        file_names_gm = [\n",
    "            (os.path.join(\n",
    "                    \"OAS1_%04d_MR1\",\n",
    "                    \"mwrc1OAS1_%04d_MR1_mpr_anon_fslswapdim_bet.nii.gz\")\n",
    "             % (s, s),\n",
    "             url_images, opts)\n",
    "            for s in range(1, 457) if s not in missing_subjects][:n_subjects]\n",
    "        file_names_wm = [\n",
    "            (os.path.join(\n",
    "                    \"OAS1_%04d_MR1\",\n",
    "                    \"mwrc2OAS1_%04d_MR1_mpr_anon_fslswapdim_bet.nii.gz\")\n",
    "             % (s, s),\n",
    "             url_images, opts)\n",
    "            for s in range(1, 457) if s not in missing_subjects]\n",
    "    else:\n",
    "        # only one gross outlier produced, hidden by nilearn API\n",
    "        removed_outliers = [390]\n",
    "        missing_subjects = sorted(missing_subjects + removed_outliers)\n",
    "        file_names_gm = [\n",
    "            (os.path.join(\n",
    "                    \"OAS1_%04d_MR1\",\n",
    "                    \"mwc1OAS1_%04d_MR1_mpr_anon_fslswapdim_bet.nii.gz\")\n",
    "             % (s, s),\n",
    "             url_images, opts)\n",
    "            for s in range(1, 457) if s not in missing_subjects][:n_subjects]\n",
    "        file_names_wm = [\n",
    "            (os.path.join(\n",
    "                    \"OAS1_%04d_MR1\",\n",
    "                    \"mwc2OAS1_%04d_MR1_mpr_anon_fslswapdim_bet.nii.gz\")\n",
    "             % (s, s),\n",
    "             url_images, opts)\n",
    "            for s in range(1, 457) if s not in missing_subjects]\n",
    "    file_names_extvars = [(\"oasis_cross-sectional.csv\", url_csv, {})]\n",
    "    file_names_dua = [(\"data_usage_agreement.txt\", url_dua, {})]\n",
    "    # restrict to user-specified number of subjects\n",
    "    file_names_gm = file_names_gm[:n_subjects]\n",
    "    file_names_wm = file_names_wm[:n_subjects]\n",
    "\n",
    "    file_names = (file_names_gm + file_names_wm +\n",
    "                  file_names_extvars + file_names_dua)\n",
    "    dataset_name = 'oasis1'\n",
    "    data_dir = _get_dataset_dir(dataset_name, data_dir=data_dir,\n",
    "                                verbose=verbose)\n",
    "    files = _fetch_files(data_dir, file_names, resume=resume,\n",
    "                         verbose=verbose)\n",
    "    print(files)\n",
    "    # Build Bunch\n",
    "    gm_maps = files[:n_subjects]\n",
    "    wm_maps = files[n_subjects:(2 * n_subjects)]\n",
    "    ext_vars_file = files[-2]\n",
    "    data_usage_agreement = files[-1]\n",
    "\n",
    "    # Keep CSV information only for selected subjects\n",
    "    csv_data = np.recfromcsv(ext_vars_file)\n",
    "    # Comparisons to recfromcsv data must be bytes.\n",
    "    actual_subjects_ids = [(\"OAS1\" +\n",
    "                            str.split(os.path.basename(x),\n",
    "                                      \"OAS1\")[1][:9]).encode()\n",
    "                           for x in gm_maps]\n",
    "    subject_mask = np.asarray([subject_id in actual_subjects_ids\n",
    "                               for subject_id in csv_data['id']])\n",
    "    csv_data = csv_data[subject_mask]\n",
    "\n",
    "    fdescr = _get_dataset_descr(dataset_name)\n",
    "\n",
    "    return Bunch(\n",
    "        gray_matter_maps=gm_maps,\n",
    "        white_matter_maps=wm_maps,\n",
    "        ext_vars=csv_data,\n",
    "        data_usage_agreement=data_usage_agreement,\n",
    "description=fdescr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Oasis dataset\n",
    "-------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     7
    ]
   },
   "outputs": [],
   "source": [
    "# oasis_dataset = fetch_oasis_vbm(n_subjects=n_subjects, dartel_version=\"false\")\n",
    "# gray_matter_map_filenames = oasis_dataset.gray_matter_maps\n",
    "# age = oasis_dataset.ext_vars['age'].astype(float)\n",
    "\n",
    "\n",
    "data_prefix = '/Users/RichardMin/nilearn_data/oasis2/'\n",
    "image_postfix = 'RAW/mpr-1.nifti.img'\n",
    "gray_matter_map_filenames = ['OAS2_0001_MR1',\n",
    "            'OAS2_0002_MR1',\n",
    "            'OAS2_0004_MR1',\n",
    "            'OAS2_0005_MR1',\n",
    "            'OAS2_0007_MR1',\n",
    "            'OAS2_0008_MR1',\n",
    "            'OAS2_0009_MR1',\n",
    "            'OAS2_0010_MR1',\n",
    "            'OAS2_0012_MR1',\n",
    "            'OAS2_0013_MR1',\n",
    "            'OAS2_0014_MR1',\n",
    "            'OAS2_0016_MR1',\n",
    "            'OAS2_0017_MR1',\n",
    "            'OAS2_0018_MR1',\n",
    "            'OAS2_0020_MR1',\n",
    "            'OAS2_0021_MR1',\n",
    "            'OAS2_0022_MR1',\n",
    "            'OAS2_0023_MR1',\n",
    "            'OAS2_0026_MR1',\n",
    "            'OAS2_0027_MR1',\n",
    "            'OAS2_0028_MR1',\n",
    "            'OAS2_0029_MR1',\n",
    "            'OAS2_0030_MR1',\n",
    "            'OAS2_0031_MR1',\n",
    "            'OAS2_0032_MR1',\n",
    "            'OAS2_0034_MR1',\n",
    "            'OAS2_0035_MR1',\n",
    "            'OAS2_0036_MR1',\n",
    "            'OAS2_0037_MR1',\n",
    "            'OAS2_0039_MR1',\n",
    "            'OAS2_0040_MR1',\n",
    "            'OAS2_0041_MR1',\n",
    "            'OAS2_0042_MR1',\n",
    "            'OAS2_0043_MR1',\n",
    "            'OAS2_0044_MR1',\n",
    "            'OAS2_0045_MR1',\n",
    "            'OAS2_0046_MR1',\n",
    "            'OAS2_0047_MR1',\n",
    "            'OAS2_0048_MR1',\n",
    "            'OAS2_0049_MR1',\n",
    "            'OAS2_0050_MR1',\n",
    "            'OAS2_0051_MR1',\n",
    "            'OAS2_0052_MR1',\n",
    "            'OAS2_0053_MR1',\n",
    "            'OAS2_0054_MR1',\n",
    "            'OAS2_0055_MR1',\n",
    "            'OAS2_0056_MR1',\n",
    "            'OAS2_0057_MR1',\n",
    "            'OAS2_0058_MR1',\n",
    "            'OAS2_0060_MR1',\n",
    "            'OAS2_0061_MR1',\n",
    "            'OAS2_0062_MR1',\n",
    "            'OAS2_0063_MR1',\n",
    "            'OAS2_0064_MR1',\n",
    "            'OAS2_0066_MR1',\n",
    "            'OAS2_0067_MR1',\n",
    "            'OAS2_0068_MR1',\n",
    "            'OAS2_0069_MR1',\n",
    "            'OAS2_0070_MR1',\n",
    "            'OAS2_0071_MR1',\n",
    "            'OAS2_0073_MR1',\n",
    "            'OAS2_0075_MR1',\n",
    "            'OAS2_0076_MR1',\n",
    "            'OAS2_0077_MR1',\n",
    "            'OAS2_0078_MR1',\n",
    "            'OAS2_0079_MR1',\n",
    "            'OAS2_0080_MR1',\n",
    "            'OAS2_0081_MR1',\n",
    "            'OAS2_0085_MR1',\n",
    "            'OAS2_0086_MR1',\n",
    "            'OAS2_0087_MR1',\n",
    "            'OAS2_0088_MR1',\n",
    "            'OAS2_0089_MR1',\n",
    "            'OAS2_0090_MR1',\n",
    "            'OAS2_0091_MR1',\n",
    "            'OAS2_0092_MR1',\n",
    "            'OAS2_0094_MR1',\n",
    "            'OAS2_0095_MR1',\n",
    "            'OAS2_0096_MR1',\n",
    "            'OAS2_0097_MR1',\n",
    "            'OAS2_0098_MR1',\n",
    "            'OAS2_0099_MR1',\n",
    "            'OAS2_0100_MR1',\n",
    "            'OAS2_0101_MR1',\n",
    "            'OAS2_0102_MR1',\n",
    "            'OAS2_0103_MR1',\n",
    "            'OAS2_0104_MR1',\n",
    "            'OAS2_0105_MR1',\n",
    "            'OAS2_0106_MR1',\n",
    "            'OAS2_0108_MR1',\n",
    "            'OAS2_0109_MR1',\n",
    "            'OAS2_0111_MR1',\n",
    "            'OAS2_0112_MR1',\n",
    "            'OAS2_0113_MR1',\n",
    "            'OAS2_0114_MR1',\n",
    "            'OAS2_0116_MR1',\n",
    "            'OAS2_0117_MR1',\n",
    "            'OAS2_0118_MR1',\n",
    "            'OAS2_0119_MR1',\n",
    "            'OAS2_0120_MR1',\n",
    "            'OAS2_0121_MR1',\n",
    "            'OAS2_0122_MR1',\n",
    "            'OAS2_0124_MR1',\n",
    "            'OAS2_0126_MR1',\n",
    "            'OAS2_0127_MR1',\n",
    "            'OAS2_0128_MR1',\n",
    "            'OAS2_0129_MR1',\n",
    "            'OAS2_0131_MR1',\n",
    "            'OAS2_0133_MR1',\n",
    "            'OAS2_0134_MR1',\n",
    "            'OAS2_0135_MR1',\n",
    "            'OAS2_0137_MR1',\n",
    "            'OAS2_0138_MR1',\n",
    "            'OAS2_0139_MR1',\n",
    "            'OAS2_0140_MR1',\n",
    "            'OAS2_0141_MR1',\n",
    "            'OAS2_0142_MR1',\n",
    "            'OAS2_0143_MR1',\n",
    "            'OAS2_0144_MR1',\n",
    "            'OAS2_0145_MR1',\n",
    "            'OAS2_0146_MR1',\n",
    "            'OAS2_0147_MR1',\n",
    "            'OAS2_0149_MR1',\n",
    "            'OAS2_0150_MR1',\n",
    "            'OAS2_0152_MR1',\n",
    "            'OAS2_0154_MR1',\n",
    "            'OAS2_0156_MR1',\n",
    "            'OAS2_0157_MR1',\n",
    "            'OAS2_0158_MR1',\n",
    "            'OAS2_0159_MR1',\n",
    "            'OAS2_0160_MR1',\n",
    "            'OAS2_0161_MR1',\n",
    "            'OAS2_0162_MR1',\n",
    "            'OAS2_0164_MR1',\n",
    "            'OAS2_0165_MR1',\n",
    "            'OAS2_0169_MR1',\n",
    "            'OAS2_0171_MR1',\n",
    "            'OAS2_0172_MR1',\n",
    "            'OAS2_0174_MR1',\n",
    "            'OAS2_0175_MR1',\n",
    "            'OAS2_0176_MR1',\n",
    "            'OAS2_0177_MR1',\n",
    "            'OAS2_0178_MR1',\n",
    "            'OAS2_0179_MR1',\n",
    "            'OAS2_0181_MR1',\n",
    "            'OAS2_0182_MR1',\n",
    "            'OAS2_0183_MR1',\n",
    "            'OAS2_0184_MR1',\n",
    "            'OAS2_0185_MR1',\n",
    "            'OAS2_0186_MR1'\n",
    "        ]\n",
    "gray_matter_map_filenames = list(map(lambda x: x**2, gray_matter_Map))\n",
    "\n",
    "csv_data = np.recfromcsv('/Users/RichardMin/CS168/oasis_longitudinal_filtered.csv')\n",
    "\n",
    "# note that we have to filter out the non MR1 scans.\n",
    "\n",
    "age = np.asarray([100. if x.decode() == 'Nondemented' else 1. if x.decode() == 'Demented' else 50. for x in csv_data['group']])\n",
    "assert(len(gray_matter_map_filenames) == len(age))\n",
    "# Comparisons to recfromcsv data must be/Users/RichardMin/nilearn_data/oasis2/oasis_longitudinal.csv bytes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess data\n",
    "----------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File not found: 'OAS2_0001_MR1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-1fe623059ca4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#     print(image.get_data().shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#     pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mgm_maps_masked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnifti_masker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray_matter_map_filenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgm_maps_masked\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%d samples, %d features\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_subjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/nilearn/input_data/base_masker.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, confounds, **fit_params)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_img\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m                 return self.fit(X, **fit_params\n\u001b[0m\u001b[1;32m    204\u001b[0m                                 ).transform(X, confounds=confounds)\n\u001b[1;32m    205\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/nilearn/input_data/nifti_masker.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, imgs, y)\u001b[0m\n\u001b[1;32m    230\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[%s.fit] Computing the mask\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             self.mask_img_ = self._cache(compute_mask, ignore=['verbose'])(\n\u001b[0;32m--> 232\u001b[0;31m                 imgs, verbose=max(0, self.verbose - 1), **mask_args)\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_img_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_niimg_3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py\u001b[0m in \u001b[0;36m_cached_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    508\u001b[0m                           \u001b[0;34m'directory %s'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m                         % (name, argument_hash, output_dir))\n\u001b[0;32m--> 510\u001b[0;31m             \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmmap_mode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m                 \u001b[0;31m# Memmap the output at the first call to be consistent with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persist_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         \u001b[0mduration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/nilearn/masking.py\u001b[0m in \u001b[0;36mcompute_background_mask\u001b[0;34m(data_imgs, border_size, connected, opening, target_affine, target_shape, memory, verbose)\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Background mask computation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m     \u001b[0mdata_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_niimg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;31m# Delayed import to avoid circular imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/nilearn/_utils/niimg_conversions.py\u001b[0m in \u001b[0;36mcheck_niimg\u001b[0;34m(niimg, ensure_ndim, atleast_4d, dtype, return_iterator, wildcards)\u001b[0m\n\u001b[1;32m    266\u001b[0m             return _iter_check_niimg(niimg, ensure_ndim=ensure_ndim,\n\u001b[1;32m    267\u001b[0m                                      dtype=dtype)\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconcat_niimgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mniimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;31m# Otherwise, it should be a filename or a SpatialImage, we load it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/nilearn/_utils/niimg_conversions.py\u001b[0m in \u001b[0;36mconcat_niimgs\u001b[0;34m(niimgs, dtype, ensure_ndim, memory, memory_level, auto_resample, verbose)\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mliterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtee\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mniimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mfirst_niimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_niimg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mliterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot concatenate empty objects'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/nilearn/_utils/niimg_conversions.py\u001b[0m in \u001b[0;36mcheck_niimg\u001b[0;34m(niimg, ensure_ndim, atleast_4d, dtype, return_iterator, wildcards)\u001b[0m\n\u001b[1;32m    257\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File not found: '%s'\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mniimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mniimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File not found: '%s'\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mniimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: File not found: 'OAS2_0001_MR1'"
     ]
    }
   ],
   "source": [
    "\n",
    "import nibabel as nib\n",
    "nifti_masker = NiftiMasker(\n",
    "    standardize=False,\n",
    "    smoothing_fwhm=2,\n",
    "    memory='nilearn_cache')  # cache options\n",
    "# gray_matter = []\n",
    "# for filename in gray_matter_map_filenames:\n",
    "#     image = nib.load(filename)\n",
    "# #     image.data.resize(176, 256, 256, 1)\n",
    "#     gray_matter.append(image)\n",
    "#     print(image.get_data().shape)\n",
    "#     pass\n",
    "gm_maps_masked = nifti_masker.fit_transform(gray_matter_map_filenames)\n",
    "n_samples, n_features = gm_maps_masked.shape\n",
    "print(\"%d samples, %d features\" % (n_subjects, n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction pipeline with ANOVA and SVR\n",
    "---------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANOVA + SVR\n"
     ]
    }
   ],
   "source": [
    "print(\"ANOVA + SVR\")\n",
    "# Define the prediction function to be used.\n",
    "# Here we use a Support Vector Classification, with a linear kernel\n",
    "from sklearn.svm import SVR\n",
    "svr = SVR(kernel='linear')\n",
    "\n",
    "# Dimension reduction\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, \\\n",
    "        f_regression\n",
    "\n",
    "# Remove features with too low between-subject variance\n",
    "variance_threshold = VarianceThreshold(threshold=.01)\n",
    "\n",
    "# Here we use a classical univariate feature selection based on F-test,\n",
    "# namely Anova.\n",
    "feature_selection = SelectKBest(f_regression, k=2000)\n",
    "\n",
    "# We have our predictor (SVR), our feature selection (SelectKBest), and now,\n",
    "# we can plug them together in a *pipeline* that performs the two operations\n",
    "# successively:\n",
    "from sklearn.pipeline import Pipeline\n",
    "anova_svr = Pipeline([\n",
    "            ('variance_threshold', variance_threshold),\n",
    "            ('anova', feature_selection),\n",
    "            ('svr', svr)])\n",
    "\n",
    "### Fit and predict\n",
    "anova_svr.fit(gm_maps_masked, age)\n",
    "age_pred = anova_svr.predict(gm_maps_masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization\n",
    "--------------\n",
    "Look at the SVR's discriminating weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py:107: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ANOVA ===\n",
      "Prediction accuracy: -0.502144\n",
      "\n",
      "Massively univariate model\n"
     ]
    }
   ],
   "source": [
    "coef = svr.coef_\n",
    "# reverse feature selection\n",
    "coef = feature_selection.inverse_transform(coef)\n",
    "# reverse variance threshold\n",
    "coef = variance_threshold.inverse_transform(coef)\n",
    "# reverse masking\n",
    "weight_img = nifti_masker.inverse_transform(coef)\n",
    "\n",
    "# Create the figure\n",
    "from nilearn.plotting import plot_stat_map, show\n",
    "bg_filename = gray_matter_map_filenames[0]\n",
    "z_slice = 0\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(5.5, 7.5), facecolor='k')\n",
    "# Hard setting vmax to highlight weights more\n",
    "display = plot_stat_map(weight_img, bg_img=bg_filename,\n",
    "                        display_mode='z', cut_coords=[z_slice],\n",
    "                        figure=fig, vmax=1)\n",
    "display.title('SVM weights', y=1.2)\n",
    "\n",
    "# Measure accuracy with cross validation\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "cv_scores = cross_val_score(anova_svr, gm_maps_masked, age)\n",
    "\n",
    "# Return the corresponding mean prediction accuracy\n",
    "prediction_accuracy = np.mean(cv_scores)\n",
    "print(\"=== ANOVA ===\")\n",
    "print(\"Prediction accuracy: %f\" % prediction_accuracy)\n",
    "print(\"\")\n",
    "\n",
    "### Inference with massively univariate model #################################\n",
    "print(\"Massively univariate model\")\n",
    "\n",
    "# Statistical inference\n",
    "from nilearn.mass_univariate import permuted_ols\n",
    "data = variance_threshold.fit_transform(gm_maps_masked)\n",
    "neg_log_pvals, t_scores_original_data, _ = permuted_ols(\n",
    "    age, data,  # + intercept as a covariate by default\n",
    "    n_perm=2000,  # 1,000 in the interest of time; 10000 would be better\n",
    "    n_jobs=1)  # can be changed to use more CPUs\n",
    "signed_neg_log_pvals = neg_log_pvals * np.sign(t_scores_original_data)\n",
    "signed_neg_log_pvals_unmasked = nifti_masker.inverse_transform(\n",
    "    variance_threshold.inverse_transform(signed_neg_log_pvals))\n",
    "\n",
    "# Show results\n",
    "threshold = -np.log10(0.1)  # 10% corrected\n",
    "\n",
    "fig = plt.figure(figsize=(5.5, 7.5), facecolor='k')\n",
    "\n",
    "display = plot_stat_map(signed_neg_log_pvals_unmasked, bg_img=bg_filename,\n",
    "                        threshold=threshold, cmap=plt.cm.RdBu_r,\n",
    "                        display_mode='z', cut_coords=[z_slice],\n",
    "                        figure=fig)\n",
    "title = ('Negative $\\log_{10}$ p-values'\n",
    "         '\\n(Non-parametric + max-type correction)')\n",
    "display.title(title, y=1.2)\n",
    "\n",
    "n_detections = (signed_neg_log_pvals_unmasked.get_data() > threshold).sum()\n",
    "print('\\n%d detections' % n_detections)\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
